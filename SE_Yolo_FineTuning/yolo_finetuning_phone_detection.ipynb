{"cells":[{"cell_type":"markdown","metadata":{"id":"0"},"source":["# YOLO Fine-tuning on Mobile Phone Dataset\n","## Fine-tune YOLOv8 and Upload to Hugging Face\n","\n","This notebook provides a complete pipeline for:\n","1. Downloading the mobile phone dataset from Kaggle\n","2. Converting Pascal VOC annotations to YOLO format\n","3. Fine-tuning YOLOv8 on the dataset\n","4. Pushing the model to Hugging Face Hub"]},{"cell_type":"markdown","metadata":{"id":"1"},"source":["## Step 1: Install Dependencies"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186737597,"user_tz":-330,"elapsed":6335,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"ce382c83-aecd-4812-b21a-796c16389d29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (26.0)\n","Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.4.11)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.90)\n","Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (1.3.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n","Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.28.1)\n","Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.5.4)\n","Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.21.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.12.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (2026.1.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub) (8.3.1)\n","YOLOv8 version: 8.4.11\n"]}],"source":["# Install required packages\n","!pip install --upgrade pip\n","!pip install ultralytics opencv-python kagglehub huggingface-hub pillow torch torchvision pyyaml\n","\n","# Verify installations\n","import ultralytics\n","print(f\"YOLOv8 version: {ultralytics.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"3"},"source":["## Step 2: Setup Kaggle Credentials\n","\n","Choose one of two methods:\n","- **Method 1**: Paste your Kaggle API token directly\n","- **Method 2**: Upload kaggle.json file"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186741799,"user_tz":-330,"elapsed":3473,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"72eafc6d-522f-4188-8832-aeae87899588"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","KAGGLE AUTHENTICATION\n","============================================================\n","\n","Choose your authentication method:\n","  1. Paste Kaggle API Token (RECOMMENDED)\n","  2. Upload kaggle.json file\n","\n","Your token will NOT be saved or displayed.\n","\n","Enter choice (1 or 2): 1\n","\n","ğŸ“ Enter your Kaggle API token\n","   Get it from: https://www.kaggle.com/settings/account\n","   Click 'Create New API Token' and copy the token value\n","\n","\n","Kaggle credentials configured using API token!\n","   Token is stored in memory for this session only.\n","\n","============================================================\n"]}],"source":["import os\n","import getpass\n","from google.colab import files\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"KAGGLE AUTHENTICATION\")\n","print(\"=\"*60)\n","print(\"\\nChoose your authentication method:\")\n","print(\"  1. Paste Kaggle API Token (RECOMMENDED)\")\n","print(\"  2. Upload kaggle.json file\")\n","print(\"\\nYour token will NOT be saved or displayed.\")\n","\n","choice = input(\"\\nEnter choice (1 or 2): \").strip()\n","\n","if choice == \"1\":\n","    print(\"\\nğŸ“ Enter your Kaggle API token\")\n","    print(\"   Get it from: https://www.kaggle.com/settings/account\")\n","    print(\"   Click 'Create New API Token' and copy the token value\\n\")\n","\n","    kaggle_token = \"\"\n","\n","    if not kaggle_token:\n","        raise ValueError(\"Token cannot be empty!\")\n","\n","    os.environ['KAGGLE_API_TOKEN'] = kaggle_token\n","\n","    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n","\n","    print(\"\\nKaggle credentials configured using API token!\")\n","    print(\"   Token is stored in memory for this session only.\")\n","\n","elif choice == \"2\":\n","    print(\"\\nğŸ“¤ Please upload your kaggle.json file\")\n","    print(\"   Get it from: https://www.kaggle.com/settings/account\")\n","    print(\"   Click 'Create New API Token' to download kaggle.json\\n\")\n","\n","    uploaded = files.upload()\n","\n","    if 'kaggle.json' not in uploaded:\n","        raise FileNotFoundError(\"kaggle.json file not found!\")\n","\n","    # Setup Kaggle\n","    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n","    !mv kaggle.json ~/.kaggle/\n","    !chmod 600 ~/.kaggle/kaggle.json\n","\n","    print(\"\\nKaggle credentials configured using kaggle.json!\")\n","else:\n","    raise ValueError(\"Invalid choice. Please enter 1 or 2.\")\n","\n","print(\"\\n\" + \"=\"*60)"]},{"cell_type":"markdown","metadata":{"id":"5"},"source":["## Step 3: Download Dataset from Kaggle"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186766303,"user_tz":-330,"elapsed":4714,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"3fbe8896-9e6f-474b-f588-3c523d4bb6aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Downloading mobile phone dataset from Kaggle...\n","Using Colab cache for faster access to the 'mobile-phone-image-dataset' dataset.\n","\n","Dataset downloaded successfully!\n","Path: /kaggle/input/mobile-phone-image-dataset\n","\n","Dataset contents:\n","  Mobile_image: 1 files\n","  Annotations: 1 files\n"]}],"source":["import kagglehub\n","import os\n","\n","print(\"\\n Downloading mobile phone dataset from Kaggle...\")\n","\n","try:\n","    dataset_path = kagglehub.dataset_download(\"dataclusterlabs/mobile-phone-image-dataset\")\n","    print(f\"\\nDataset downloaded successfully!\")\n","    print(f\"Path: {dataset_path}\")\n","except Exception as e:\n","    print(f\"\\nError downloading dataset: {str(e)}\")\n","    print(\"\\nTroubleshooting:\")\n","    print(\"  1. Verify your Kaggle API token is correct\")\n","    print(\"  2. Check that you have access to the dataset\")\n","    print(\"  3. Ensure your internet connection is stable\")\n","    raise\n","\n","print(\"\\nDataset contents:\")\n","for item in os.listdir(dataset_path):\n","    item_path = os.path.join(dataset_path, item)\n","    if os.path.isdir(item_path):\n","        file_count = len(os.listdir(item_path))\n","        print(f\"  {item}: {file_count} files\")\n","    else:\n","        print(f\"  {item}\")"]},{"cell_type":"code","source":["import os\n","\n","# Check what's actually in the Annotations directory\n","annotations_dir = \"/kaggle/input/mobile-phone-image-dataset/Annotations/Annotations\"\n","\n","print(\"=\" * 70)\n","print(\"CHECKING ANNOTATION DIRECTORY\")\n","print(\"=\" * 70)\n","\n","print(f\"\\nPath: {annotations_dir}\")\n","print(f\"Exists: {os.path.exists(annotations_dir)}\")\n","\n","if os.path.exists(annotations_dir):\n","    items = os.listdir(annotations_dir)\n","    print(f\"Total items: {len(items)}\\n\")\n","\n","    print(\"First 20 items:\")\n","    for item in items[:20]:\n","        item_path = os.path.join(annotations_dir, item)\n","        if os.path.isdir(item_path):\n","            sub_count = len(os.listdir(item_path))\n","            print(f\"  {item}/ ({sub_count} files)\")\n","        else:\n","            _, ext = os.path.splitext(item)\n","            print(f\"  {item} ({ext})\")\n","\n","    # Count file types\n","    xml_count = len([f for f in items if f.endswith('.xml')])\n","    json_count = len([f for f in items if f.endswith('.json')])\n","\n","    print(f\"\\nSummary:\")\n","    print(f\"  XML files: {xml_count}\")\n","    print(f\"  JSON files: {json_count}\")\n","    print(f\"  Directories: {len([f for f in items if os.path.isdir(os.path.join(annotations_dir, f))])}\")\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"FULL DIRECTORY TREE\")\n","print(\"=\" * 70)\n","\n","for root, dirs, files in os.walk(annotations_dir):\n","    level = root.replace(annotations_dir, '').count(os.sep)\n","    indent = '  ' * level\n","    print(f\"{indent} {os.path.basename(root)}/\")\n","\n","    for f in files[:10]:\n","        print(f\"{indent}    {f}\")\n","    if len(files) > 10:\n","        print(f\"{indent}  ... and {len(files) - 10} more\")\n","\n","    if level > 2:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYvCq-uzNpxh","executionInfo":{"status":"ok","timestamp":1770186798429,"user_tz":-330,"elapsed":52,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"f6b49041-4f10-47cc-8f63-43b1679d8b44"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","CHECKING ANNOTATION DIRECTORY\n","======================================================================\n","\n","Path: /kaggle/input/mobile-phone-image-dataset/Annotations/Annotations\n","Exists: True\n","Total items: 100\n","\n","First 20 items:\n","  Datacluster Labs Phone Dataset (49).xml (.xml)\n","  Datacluster Labs Phone Dataset (47).xml (.xml)\n","  Datacluster Labs Phone Dataset (65).xml (.xml)\n","  Datacluster Labs Phone Dataset (5).xml (.xml)\n","  Datacluster Labs Phone Dataset (94).xml (.xml)\n","  Datacluster Labs Phone Dataset (14).xml (.xml)\n","  Datacluster Labs Phone Dataset (4).xml (.xml)\n","  Datacluster Labs Phone Dataset (45).xml (.xml)\n","  Datacluster Labs Phone Dataset (90).xml (.xml)\n","  Datacluster Labs Phone Dataset (43).xml (.xml)\n","  Datacluster Labs Phone Dataset (30).xml (.xml)\n","  Datacluster Labs Phone Dataset (22).xml (.xml)\n","  Datacluster Labs Phone Dataset (15).xml (.xml)\n","  Datacluster Labs Phone Dataset (13).xml (.xml)\n","  Datacluster Labs Phone Dataset (19).xml (.xml)\n","  Datacluster Labs Phone Dataset (92).xml (.xml)\n","  Datacluster Labs Phone Dataset (23).xml (.xml)\n","  Datacluster Labs Phone Dataset (1).xml (.xml)\n","  Datacluster Labs Phone Dataset (3).xml (.xml)\n","  Datacluster Labs Phone Dataset (97).xml (.xml)\n","\n","Summary:\n","  XML files: 100\n","  JSON files: 0\n","  Directories: 0\n","\n","======================================================================\n","FULL DIRECTORY TREE\n","======================================================================\n"," Annotations/\n","    Datacluster Labs Phone Dataset (49).xml\n","    Datacluster Labs Phone Dataset (47).xml\n","    Datacluster Labs Phone Dataset (65).xml\n","    Datacluster Labs Phone Dataset (5).xml\n","    Datacluster Labs Phone Dataset (94).xml\n","    Datacluster Labs Phone Dataset (14).xml\n","    Datacluster Labs Phone Dataset (4).xml\n","    Datacluster Labs Phone Dataset (45).xml\n","    Datacluster Labs Phone Dataset (90).xml\n","    Datacluster Labs Phone Dataset (43).xml\n","  ... and 90 more\n"]}]},{"cell_type":"markdown","metadata":{"id":"7"},"source":["## Step 4: Convert Pascal VOC to YOLO Format"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186834866,"user_tz":-330,"elapsed":4067,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"669b8094-4748-49d5-dab4-5fe3a5e0f424"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  Converting Pascal VOC to YOLO format...\n","\n","  Annotations directory: /kaggle/input/mobile-phone-image-dataset/Annotations/Annotations\n","  Images directory: /kaggle/input/mobile-phone-image-dataset/Mobile_image\n","\n","  Scanning dataset...\n","Found 100 annotation files\n","\n","  Dataset split:\n","  Train: 80 samples\n","  Val:   10 samples\n","  Test:  10 samples\n","\n","Processing train set...\n","  train set processed\n","\n","Processing val set...\n","  val set processed\n","\n","Processing test set...\n","  test set processed\n","\n","\n","Conversion complete!\n","  Successfully converted: 100\n","  Skipped: 0\n","  Dataset: /content/yolo_dataset\n"]}],"source":["import os\n","import xml.etree.ElementTree as ET\n","from pathlib import Path\n","import shutil\n","import random\n","\n","# Configuration\n","DATASET_PATH = dataset_path\n","OUTPUT_DIR = \"/content/yolo_dataset\"\n","TRAIN_RATIO = 0.8\n","VAL_RATIO = 0.1\n","TEST_RATIO = 0.1\n","\n","print(\"\\n  Converting Pascal VOC to YOLO format...\\n\")\n","\n","# Create output directories\n","os.makedirs(f\"{OUTPUT_DIR}/images/train\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/images/val\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/images/test\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/labels/train\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/labels/val\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/labels/test\", exist_ok=True)\n","\n","# Class mapping\n","class_mapping = {'mobile_phone': 0}\n","\n","def convert_voc_to_yolo(xml_file, image_width, image_height):\n","    \"\"\"\n","    Convert VOC bounding box to YOLO format\n","    VOC format: xmin, ymin, xmax, ymax (pixel coordinates)\n","    YOLO format: x_center, y_center, width, height (normalized 0-1)\n","    \"\"\"\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","\n","    yolo_annotations = []\n","\n","    for obj in root.findall('object'):\n","        class_name = obj.find('name').text\n","        if class_name not in class_mapping:\n","            continue\n","\n","        class_id = class_mapping[class_name]\n","\n","        bndbox = obj.find('bndbox')\n","        xmin = float(bndbox.find('xmin').text)\n","        ymin = float(bndbox.find('ymin').text)\n","        xmax = float(bndbox.find('xmax').text)\n","        ymax = float(bndbox.find('ymax').text)\n","\n","        # Convert to YOLO format\n","        x_center = (xmin + xmax) / 2.0 / image_width\n","        y_center = (ymin + ymax) / 2.0 / image_height\n","        width = (xmax - xmin) / image_width\n","        height = (ymax - ymin) / image_height\n","\n","        # Clamp values to [0, 1]\n","        x_center = max(0, min(1, x_center))\n","        y_center = max(0, min(1, y_center))\n","        width = max(0, min(1, width))\n","        height = max(0, min(1, height))\n","\n","        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n","\n","    return yolo_annotations\n","\n","# Find annotation and image directories\n","annotations_dir = \"/kaggle/input/mobile-phone-image-dataset/Annotations/Annotations\"\n","images_source_dir = \"/kaggle/input/mobile-phone-image-dataset/Mobile_image\"\n","\n","print(f\"  Annotations directory: {annotations_dir}\")\n","print(f\"  Images directory: {images_source_dir}\\n\")\n","\n","if not annotations_dir or not images_source_dir:\n","    print(\"   ERROR: Could not find annotations or images directory!\")\n","    print(f\"\\nContents of {DATASET_PATH}:\")\n","    print(os.listdir(DATASET_PATH))\n","    raise FileNotFoundError(\"Dataset directories not found\")\n","else:\n","    print(\"  Scanning dataset...\")\n","\n","    # Collect all XML files\n","    xml_files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n","    print(f\"Found {len(xml_files)} annotation files\\n\")\n","\n","    # Shuffle and split\n","    random.shuffle(xml_files)\n","    train_count = int(len(xml_files) * TRAIN_RATIO)\n","    val_count = int(len(xml_files) * VAL_RATIO)\n","\n","    train_files = xml_files[:train_count]\n","    val_files = xml_files[train_count:train_count + val_count]\n","    test_files = xml_files[train_count + val_count:]\n","\n","    print(f\"  Dataset split:\")\n","    print(f\"  Train: {len(train_files)} samples\")\n","    print(f\"  Val:   {len(val_files)} samples\")\n","    print(f\"  Test:  {len(test_files)} samples\\n\")\n","\n","    conversion_count = 0\n","    skipped_count = 0\n","\n","    annotations_dir = None\n","    images_source_dir = None\n","\n","    for item in os.listdir(DATASET_PATH):\n","      item_path = os.path.join(DATASET_PATH, item)\n","      if os.path.isdir(item_path):\n","          if 'annotation' in item.lower():\n","              # Check if there's a nested Annotations/Annotations structure\n","              potential_nested = os.path.join(item_path, item)\n","              if os.path.exists(potential_nested) and os.path.isdir(potential_nested):\n","                  annotations_dir = potential_nested\n","              else:\n","                  annotations_dir = item_path\n","\n","          if 'image' in item.lower() or 'mobile' in item.lower():\n","              # Check if there's a nested Mobile_image/Mobile_image structure\n","              potential_nested = os.path.join(item_path, item)\n","              if os.path.exists(potential_nested) and os.path.isdir(potential_nested):\n","                  images_source_dir = potential_nested\n","              else:\n","                  images_source_dir = item_path\n","\n","\n","    conversion_count = 0\n","    skipped_count = 0\n","    error_count = 0\n","\n","    for split, split_files in [(\"train\", train_files), (\"val\", val_files), (\"test\", test_files)]:\n","        print(f\"Processing {split} set...\")\n","        for xml_file in split_files:\n","            try:\n","                xml_path = os.path.join(annotations_dir, xml_file)\n","\n","                # Get image filename from XML and image dimensions\n","                tree = ET.parse(xml_path)\n","                root = tree.getroot()\n","                image_filename = root.find('filename').text\n","\n","                size_elem = root.find('size')\n","                image_width = int(size_elem.find('width').text)\n","                image_height = int(size_elem.find('height').text)\n","\n","                # Convert annotations\n","                yolo_annotations = convert_voc_to_yolo(xml_path, image_width, image_height)\n","\n","                if not yolo_annotations:\n","                    skipped_count += 1\n","                    continue\n","\n","                # Find and copy image\n","                image_source = os.path.join(images_source_dir, image_filename)\n","                if os.path.exists(image_source):\n","                    image_dest = os.path.join(OUTPUT_DIR, \"images\", split, image_filename)\n","                    shutil.copy2(image_source, image_dest)\n","                    # Save YOLO format annotations\n","                    label_filename = os.path.splitext(image_filename)[0] + \".txt\"\n","                    label_dest = os.path.join(OUTPUT_DIR, \"labels\", split, label_filename)\n","                    with open(label_dest, 'w') as f:\n","                        f.write(\"\\n\".join(yolo_annotations))\n","\n","                    conversion_count += 1\n","                else:\n","                    error_count += 1\n","            except Exception as e:\n","                skipped_count += 1\n","\n","        print(f\"  {split} set processed\\n\")\n","\n","\n","\n","\n","    print(f\"\\nConversion complete!\")\n","    print(f\"  Successfully converted: {conversion_count}\")\n","    print(f\"  Skipped: {skipped_count}\")\n","    print(f\"  Dataset: {OUTPUT_DIR}\")"]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","\n","# Check what's in the downloaded dataset\n","print(\"=\" * 70)\n","print(\"ğŸ” DATASET DIAGNOSTIC\")\n","print(\"=\" * 70)\n","\n","print(f\"\\n  Main dataset path: {dataset_path}\\n\")\n","\n","# List everything in the dataset\n","print(\"Contents of dataset directory:\")\n","for item in os.listdir(dataset_path):\n","    item_path = os.path.join(dataset_path, item)\n","    if os.path.isdir(item_path):\n","        file_count = len(os.listdir(item_path))\n","        print(f\"\\n    {item}/ ({file_count} files)\")\n","\n","        for sub_item in os.listdir(item_path)[:5]:\n","            sub_path = os.path.join(item_path, sub_item)\n","            if os.path.isdir(sub_path):\n","                print(f\"       {sub_item}/\")\n","            else:\n","                print(f\"       {sub_item}\")\n","        if file_count > 5:\n","            print(f\"     ... and {file_count - 5} more\")\n","    else:\n","        print(f\"\\n    {item}\")\n","\n","# Show full directory tree\n","print(\"\\n\" + \"=\" * 70)\n","print(\"Full directory structure:\")\n","print(\"=\" * 70)\n","!find {dataset_path} -type d | head -30"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pveC-lIFNTJ2","executionInfo":{"status":"ok","timestamp":1770186852286,"user_tz":-330,"elapsed":446,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"cfc3830e-e711-4362-bc30-434b8e273be9"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ğŸ” DATASET DIAGNOSTIC\n","======================================================================\n","\n","  Main dataset path: /kaggle/input/mobile-phone-image-dataset\n","\n","Contents of dataset directory:\n","\n","    Mobile_image/ (1 files)\n","       Mobile_image/\n","\n","    Annotations/ (1 files)\n","       Annotations/\n","\n","======================================================================\n","Full directory structure:\n","======================================================================\n","/kaggle/input/mobile-phone-image-dataset\n","/kaggle/input/mobile-phone-image-dataset/Mobile_image\n","/kaggle/input/mobile-phone-image-dataset/Mobile_image/Mobile_image\n","/kaggle/input/mobile-phone-image-dataset/Annotations\n","/kaggle/input/mobile-phone-image-dataset/Annotations/Annotations\n"]}]},{"cell_type":"markdown","metadata":{"id":"9"},"source":["## Step 5: Verify Dataset Structure"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"10","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186859435,"user_tz":-330,"elapsed":8,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"c2a8d310-b6e5-40c4-a97a-0e5623376601"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  Dataset Structure Verification\n","\n","success TRAIN | Images:   97 | Labels:   97\n","success VAL   | Images:   19 | Labels:   19\n","success TEST  | Images:   18 | Labels:   18\n","\n","Sample annotation file:\n","  File: Datacluster Labs Phone Dataset (32).txt\n","  Content:\n","0 0.4474038461538462 0.37786057692307695 0.5378846153846154 0.7557211538461539\n"]}],"source":["import os\n","\n","print(\"\\n  Dataset Structure Verification\\n\")\n","\n","# Verify dataset structure\n","for split in ['train', 'val', 'test']:\n","    images_count = len(os.listdir(f\"{OUTPUT_DIR}/images/{split}\"))\n","    labels_count = len(os.listdir(f\"{OUTPUT_DIR}/labels/{split}\"))\n","    match = \"success\" if images_count == labels_count else \"failure\"\n","    print(f\"{match} {split.upper():5} | Images: {images_count:4d} | Labels: {labels_count:4d}\")\n","\n","# Show sample annotation\n","print(\"\\nSample annotation file:\")\n","sample_label = f\"{OUTPUT_DIR}/labels/train/{os.listdir(f'{OUTPUT_DIR}/labels/train')[0]}\"\n","with open(sample_label, 'r') as f:\n","    content = f.read()\n","    print(f\"  File: {os.path.basename(sample_label)}\")\n","    print(f\"  Content:\\n{content}\")"]},{"cell_type":"markdown","metadata":{"id":"11"},"source":["## Step 6: Create YOLO Dataset Configuration File"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186870732,"user_tz":-330,"elapsed":8,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"94e25061-2e60-4581-a3ae-316f425d98c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Dataset configuration created\n","Path: /content/yolo_dataset/data.yaml\n","\n","Configuration:\n","names:\n","- mobile_phone\n","nc: 1\n","path: /content/yolo_dataset\n","test: images/test\n","train: images/train\n","val: images/val\n","\n"]}],"source":["import yaml\n","import os\n","\n","# Dataset.yaml for YOLO\n","dataset_yaml = {\n","    'path': OUTPUT_DIR,\n","    'train': 'images/train',\n","    'val': 'images/val',\n","    'test': 'images/test',\n","    'nc': 1,\n","    'names': ['mobile_phone']\n","}\n","\n","yaml_path = os.path.join(OUTPUT_DIR, 'data.yaml')\n","with open(yaml_path, 'w') as f:\n","    yaml.dump(dataset_yaml, f)\n","\n","print(f\"\\nDataset configuration created\")\n","print(f\"Path: {yaml_path}\\n\")\n","print(\"Configuration:\")\n","print(yaml.dump(dataset_yaml))"]},{"cell_type":"markdown","metadata":{"id":"13"},"source":["## Step 7: Fine-tune YOLOv8 Model"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770187100413,"user_tz":-330,"elapsed":210591,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"39ecd11d-d2a5-4863-d8a4-223067c34ebc"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","STARTING MODEL TRAINING\n","============================================================\n","\n","Loading pretrained YOLOv8n model...\n","\n","    Training configuration:\n","  â€¢ Model: YOLOv8n (Nano)\n","  â€¢ Epochs: 50\n","  â€¢ Image size: 640x640\n","  â€¢ Batch size: 16\n","  â€¢ Early stopping patience: 10 epochs\n","  â€¢ Dataset: /content/yolo_dataset/data.yaml\n","\n","   Starting training... This may take 2-3 hours on T4 GPU\n","\n","Ultralytics 8.4.11 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=0.5, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/yolo_dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.45, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolov8n_v12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=mobile_phone_detector, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/mobile_phone_detector/yolov8n_v12, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, 16, None, [64, 128, 256]] \n","Model summary: 130 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n","\n","Transferred 319/355 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3379.5Â±659.7 MB/s, size: 2317.1 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/labels/train... 97 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 97/97 43.0it/s 2.3s\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (40).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (45).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (57).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (64).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (86).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/Datacluster Labs Phone Dataset (94).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/labels/train.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 994.3Â±1372.0 MB/s, size: 3277.3 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/labels/val... 19 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 19/19 277.6it/s 0.1s\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/labels/val.cache\n","Plotting labels to /content/runs/detect/mobile_phone_detector/yolov8n_v12/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/runs/detect/mobile_phone_detector/yolov8n_v12\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       1/50      2.22G     0.9001      2.663      1.466          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4s/it 16.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n","                   all         19         19          0          0          0          0\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       2/50      2.32G     0.5596      1.809      1.111          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.5it/s 4.8s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.9it/s 0.3s\n","                   all         19         19          0          0          0          0\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       3/50      2.32G     0.6079      1.176      1.116          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.1s/it 8.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.9it/s 0.2s\n","                   all         19         19          0          0          0          0\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       4/50      2.32G     0.5933      1.107      1.105          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.2it/s 3.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.3it/s 0.1s\n","                   all         19         19          0          0          0          0\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       5/50      2.32G     0.6252      1.072       1.11          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.6it/s 2.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.7it/s 0.3s\n","                   all         19         19          1      0.789      0.895      0.726\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       6/50      2.32G     0.5836     0.9935      1.074          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.1it/s 3.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.9it/s 0.2s\n","                   all         19         19          1      0.474      0.737      0.664\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       7/50      2.32G     0.7383      1.076      1.474          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.0it/s 2.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.7it/s 0.1s\n","                   all         19         19          1      0.211      0.605      0.545\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       8/50      2.32G     0.5854      0.937      1.043          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.7it/s 1.9s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.6it/s 0.1s\n","                   all         19         19          1      0.368      0.684      0.634\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       9/50      2.32G     0.7368      1.031      1.358          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.0it/s 2.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.9it/s 0.2s\n","                   all         19         19          1      0.158      0.579      0.539\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      10/50      2.32G       0.59     0.9273       1.11          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.7it/s 2.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.0it/s 0.1s\n","                   all         19         19          1      0.737      0.868      0.744\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      11/50      2.32G     0.6226     0.9011      1.152          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.4it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.3it/s 0.2s\n","                   all         19         19      0.909      0.526      0.727      0.635\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      12/50      2.32G     0.5559     0.8514      1.048          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.3it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.6it/s 0.1s\n","                   all         19         19      0.875      0.368      0.626      0.545\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      13/50      2.32G     0.5992     0.8447       1.06          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.6it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.2it/s 0.2s\n","                   all         19         19          1      0.526      0.763       0.57\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      14/50      2.32G     0.7001     0.9477      1.212          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.7it/s 2.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.6it/s 0.2s\n","                   all         19         19      0.929      0.684      0.816      0.672\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      15/50      2.32G     0.6141     0.7864       1.14          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.2it/s 2.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.4it/s 0.2s\n","                   all         19         19       0.83      0.771      0.903      0.794\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      16/50      2.32G     0.5461     0.7594      1.036          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.0it/s 2.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.5it/s 0.3s\n","                   all         19         19      0.892      0.842      0.896      0.776\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      17/50      2.32G      1.121      1.289      1.464          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.8it/s 2.5s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.7it/s 0.3s\n","                   all         19         19      0.807      0.789      0.829      0.647\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      18/50      2.32G     0.5958     0.8812      1.083          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.8it/s 0.2s\n","                   all         19         19      0.742      0.632      0.708      0.609\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      19/50      2.32G     0.5928     0.7282      1.084          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.3it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.9it/s 0.2s\n","                   all         19         19      0.881      0.526      0.665       0.62\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      20/50      2.32G     0.5565     0.7573      1.053          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.5it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.4it/s 0.2s\n","                   all         19         19       0.81      0.895      0.908      0.669\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      21/50      2.32G     0.6029     0.7374       1.08          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.1it/s 3.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.3it/s 0.3s\n","                   all         19         19      0.939          1      0.993      0.812\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      22/50      2.32G      0.567     0.7707      1.068          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.6it/s 1.9s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.7it/s 0.2s\n","                   all         19         19          1      0.947      0.974      0.886\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      23/50      2.32G     0.5493     0.6516      1.048          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.5it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.7it/s 0.2s\n","                   all         19         19          1          1      0.995      0.857\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      24/50      2.32G     0.6237     0.9653      1.095          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.3it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.8it/s 0.2s\n","                   all         19         19          1      0.947      0.974       0.89\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      25/50      2.32G      0.689     0.8532      1.202          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.6it/s 2.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.3it/s 0.2s\n","                   all         19         19          1      0.947      0.974      0.881\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      26/50      2.32G     0.5932     0.6358      1.051          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.5it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.4it/s 0.2s\n","                   all         19         19          1          1      0.995        0.9\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      27/50      2.32G     0.4999     0.6471      1.003          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.4it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.2it/s 0.2s\n","                   all         19         19          1          1      0.995       0.94\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      28/50      2.32G     0.5094     0.6449     0.9935          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.5it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.4it/s 0.3s\n","                   all         19         19          1          1      0.995      0.946\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      29/50      2.32G     0.4753     0.5779      1.011          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.5it/s 2.8s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 4.4it/s 0.2s\n","                   all         19         19          1          1      0.995      0.934\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      30/50      2.32G     0.4856     0.5737      1.041          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.1it/s 2.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.0it/s 0.2s\n","                   all         19         19          1          1      0.995      0.935\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      31/50      2.32G     0.4904     0.5648     0.9956          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.2it/s 2.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4it/s 0.7s\n","                   all         19         19          1          1      0.995      0.974\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      32/50      2.32G     0.4522     0.5704      1.031          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 3.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.9it/s 0.3s\n","                   all         19         19          1          1      0.995      0.967\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      33/50      2.32G     0.5187     0.6611      1.026          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.2it/s 2.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.2it/s 0.8s\n","                   all         19         19          1          1      0.995      0.946\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      34/50      2.32G     0.4433      0.523     0.9945          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.3it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3it/s 0.8s\n","                   all         19         19          1          1      0.995      0.939\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      35/50      2.32G     0.4323     0.4984     0.9854          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.4it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.0it/s 0.5s\n","                   all         19         19       0.95          1      0.995      0.966\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      36/50      2.32G     0.4569     0.5601      1.023          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.2it/s 3.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n","                   all         19         19       0.95          1       0.99       0.94\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      37/50      2.32G     0.4019     0.4753     0.9924          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.5it/s 2.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.7it/s 0.6s\n","                   all         19         19      0.947      0.947      0.967      0.918\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      38/50      2.32G     0.3965     0.5324     0.9853          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.4it/s 2.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n","                   all         19         19      0.947      0.947      0.969      0.936\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      39/50      2.32G     0.4227     0.5334      1.008          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.7it/s 0.4s\n","                   all         19         19      0.989      0.947      0.972      0.954\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      40/50      2.32G     0.4168     0.4667     0.9833          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.1it/s 2.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4it/s 0.7s\n","                   all         19         19      0.996      0.947      0.972      0.951\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      41/50      2.32G       0.43     0.8215     0.9546          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.6s/it 18.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.3it/s 0.4s\n","                   all         19         19      0.995      0.947      0.972      0.953\n","\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 31, best model saved as best.pt.\n","To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n","\n","41 epochs completed in 0.052 hours.\n","Optimizer stripped from /content/runs/detect/mobile_phone_detector/yolov8n_v12/weights/last.pt, 6.2MB\n","Optimizer stripped from /content/runs/detect/mobile_phone_detector/yolov8n_v12/weights/best.pt, 6.2MB\n","\n","Validating /content/runs/detect/mobile_phone_detector/yolov8n_v12/weights/best.pt...\n","Ultralytics 8.4.11 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","Model summary (fused): 73 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.3it/s 0.3s\n","                   all         19         19          1          1      0.995      0.966\n","Speed: 0.2ms preprocess, 3.4ms inference, 0.0ms loss, 2.5ms postprocess per image\n","Results saved to \u001b[1m/content/runs/detect/mobile_phone_detector/yolov8n_v12\u001b[0m\n","\n","============================================================\n","   TRAINING COMPLETED!\n","============================================================\n"]}],"source":["from ultralytics import YOLO\n","import os\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"STARTING MODEL TRAINING\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Load pretrained YOLOv8 model\n","print(\"Loading pretrained YOLOv8n model...\\n\")\n","model = YOLO('yolov8n.pt')  # nano model - fastest, good for mobile\n","\n","# Training configuration\n","epochs = 50  # Adjust based on your needs\n","imgsz = 640\n","batch_size = 16  # Adjust based on GPU memory\n","patience = 10  # Early stopping patience\n","\n","print(f\"    Training configuration:\")\n","print(f\"  â€¢ Model: YOLOv8n (Nano)\")\n","print(f\"  â€¢ Epochs: {epochs}\")\n","print(f\"  â€¢ Image size: {imgsz}x{imgsz}\")\n","print(f\"  â€¢ Batch size: {batch_size}\")\n","print(f\"  â€¢ Early stopping patience: {patience} epochs\")\n","print(f\"  â€¢ Dataset: {yaml_path}\\n\")\n","\n","print(\"   Starting training... This may take 2-3 hours on T4 GPU\\n\")\n","\n","# Train the model\n","results = model.train(\n","    data=yaml_path,\n","    epochs=epochs,\n","    imgsz=imgsz,\n","    batch=batch_size,\n","    patience=patience,\n","    device=0,  # GPU device\n","    project='mobile_phone_detector',\n","    name='yolov8n_v1',\n","    exist_ok=False,\n","    verbose=True,\n","    save=True,\n","    save_period=5,\n","    plots=True,\n","    conf=0.5,\n","    iou=0.45\n",")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"   TRAINING COMPLETED!\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":[],"metadata":{"id":"JS5MUQuWSOil"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15"},"source":["## Step 8: Evaluate Model"]},{"cell_type":"code","source":["import os\n","from ultralytics import YOLO\n","\n","# List what's in the working directory\n","print(\"Contents of /content:\")\n","print(os.listdir('/content'))\n","\n","# Check if mobile_phone_detector exists\n","if os.path.exists('/content/mobile_phone_detector'):\n","    print(\"\\nContents of mobile_phone_detector:\")\n","    for root, dirs, files in os.walk('/content/mobile_phone_detector'):\n","        level = root.replace('/content/mobile_phone_detector', '').count(os.sep)\n","        indent = '  ' * level\n","        print(f\"{indent}{os.path.basename(root)}/\")\n","        for file in files[:5]:\n","            print(f\"{indent}  {file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQpm_rqFSMGG","executionInfo":{"status":"ok","timestamp":1770187107282,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"4e76c375-911f-458c-9c89-1912eb3eb935"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Contents of /content:\n","['.config', 'yolov8n.pt', 'yolo26n.pt', 'yolo_dataset', 'runs', 'sample_data']\n"]}]},{"cell_type":"code","source":["import os\n","\n","print(\"Contents of /content/runs:\")\n","for root, dirs, files in os.walk('/content/runs'):\n","    level = root.replace('/content/runs', '').count(os.sep)\n","    indent = '  ' * level\n","    print(f\"{indent}{os.path.basename(root)}/\")\n","    for file in files[:10]:\n","        print(f\"{indent}  {file}\")\n","    if level > 2:  # Limit depth\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"106CzdirS4q9","executionInfo":{"status":"ok","timestamp":1770187109627,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"23dff640-4efb-4e0f-d574-c8bb100b439a"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Contents of /content/runs:\n","runs/\n","  detect/\n","    mobile_phone_detector/\n","      yolov8n_v12/\n","        args.yaml\n","        BoxF1_curve.png\n","        BoxR_curve.png\n","        train_batch282.jpg\n","        train_batch2.jpg\n","        val_batch0_labels.jpg\n","        train_batch280.jpg\n","        train_batch1.jpg\n","        confusion_matrix_normalized.png\n","        confusion_matrix.png\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","print(\"\\n  EVALUATING MODEL\\n\")\n","\n","# Load the best model\n","best_model = YOLO('/content/runs/detect/mobile_phone_detector/yolov8n_v1/weights/best.pt')\n","\n","# Validate on test set\n","print(\"Running validation on test set...\\n\")\n","metrics = best_model.val()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"   VALIDATION RESULTS\")\n","print(\"=\"*60)\n","print(f\"  mAP50 (0.50 IOU):   {metrics.box.map50:.3f}\")\n","print(f\"  mAP50-95:           {metrics.box.map:.3f}\")\n","print(f\"  Precision:          {metrics.box.mp:.3f}\")\n","print(f\"  Recall:             {metrics.box.mr:.3f}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAev7ZybS_YI","executionInfo":{"status":"ok","timestamp":1770187119282,"user_tz":-330,"elapsed":7911,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"77ce1263-e32d-4af0-ddde-fd025a79ff03"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  EVALUATING MODEL\n","\n","Running validation on test set...\n","\n","Ultralytics 8.4.11 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","Model summary (fused): 73 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2971.5Â±1716.7 MB/s, size: 4801.4 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/labels/val.cache... 19 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 19/19 8.9Mit/s 0.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.7s/it 3.5s\n","                   all         19         19      0.946          1      0.993      0.903\n","Speed: 8.9ms preprocess, 20.6ms inference, 0.0ms loss, 2.2ms postprocess per image\n","Results saved to \u001b[1m/content/runs/detect/val2\u001b[0m\n","\n","============================================================\n","   VALIDATION RESULTS\n","============================================================\n","  mAP50 (0.50 IOU):   0.993\n","  mAP50-95:           0.903\n","  Precision:          0.946\n","  Recall:             1.000\n","============================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"17"},"source":["## Step 9: Setup Hugging Face Credentials"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770187119423,"user_tz":-330,"elapsed":139,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"e64c4d37-0cab-476d-b795-986f4d31faf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","ğŸ¤— HUGGING FACE AUTHENTICATION\n","\n","  Logged in to Hugging Face successfully!\n"]}],"source":["from huggingface_hub import login\n","import getpass\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ğŸ¤— HUGGING FACE AUTHENTICATION\")\n","\n","hf_token =\"\"\n","\n","# Login to Hugging Face\n","try:\n","    login(token=hf_token)\n","    print(\"\\n  Logged in to Hugging Face successfully!\")\n","except Exception as e:\n","    print(f\"\\n  Authentication failed: {str(e)}\")\n","    raise"]},{"cell_type":"markdown","metadata":{"id":"19"},"source":["## Step 10: Push Model to Hugging Face"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"20","colab":{"base_uri":"https://localhost:8080/","height":487,"referenced_widgets":["a367a0df4ab94c5e9628ec7934283b09","b4f7bea897e74c6c80ec4033a6a61884","ecc1ceb4d8eb4966b46d3ecb2621dcd2","0186aacbfcca4cf7910f40e7341fe4b7","372915d321f846bd89793785e56830fb","47aa8b94d852435b897f396850ae6ff9","9d80e494f22a41219167cf7098725a8c","04629e83ef064c948cbba22ad4b9baf4","2b84218ad2f349fca590af58c07527a5","7076d7a07c6c4763b7b2974bd9be1f2a","ad0029a2cdde41a599f705ade3b68b3f","3b9ca9c8680449a8a5bca9e45b077cfc","9022cd2975f5457e81bbb059ddac0d67","f21d899641ed4b2eac4a3502620d80ba","2372caa2578a477fa59c6cd0f6a85492","b4adbb1cd5cf4a56a1a58af84db51aa6","98ea7305da3c4efe990ec8758f4f970c","2f964136c97944aa9dd2f087aaf82b20","aad73b3357a7499b9c34bb337f98540f","8097174c047b46cfbaaf1faa162ab6aa","a242514f57f64f03bafab092a98d6cdf","0682d5112f7742ac987d0240f109b468","9d2f65d7973b45458347a9117a83e4f9","3ea74d0245704c17892cb6318c558e88","bc661a70db0f4f37941ca4b61e2db75a","e2796ea47f8c4ee7bd1ba68acb3cc7cd","c44ba661e11b4604b9398dd64058e00a","04e75a8c97bb4368b4f38aafa162afcb","311d3fc85e6c4cc8bedf5f9bd21c0e03","46093716481946f9a92a1522de7788ed","4871ff809b8f4d8c9d7cb066ec5f0698","9b0379ae8294489d99c25c7be3f4a675","dbe8d84e0b36441192323fdb8426f019"]},"executionInfo":{"status":"ok","timestamp":1770187133610,"user_tz":-330,"elapsed":1390,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"e468ea95-87fd-43cf-982b-a971dd21364f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","  PUSHING MODEL TO HUGGING FACE\n","============================================================\n","\n","  Target repository: IndUSV/yolov8n-mobile-phone\n","\n","  Repository IndUSV/yolov8n-mobile-phone is ready\n","\n","  Model file found: /content/runs/detect/mobile_phone_detector/yolov8n_v1/weights/best.pt\n","   Size: 6.0 MB\n","\n","  Uploading model weights...\n"]},{"output_type":"display_data","data":{"text/plain":["Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a367a0df4ab94c5e9628ec7934283b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["New Data Upload               : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9ca9c8680449a8a5bca9e45b077cfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  ...olov8n_v1/weights/best.pt: 100%|##########| 6.25MB / 6.25MB            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2f65d7973b45458347a9117a83e4f9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"stream","name":"stdout","text":["   Model weights uploaded!\n","\n","  Uploading training results...\n","  Training results uploaded!\n","\n"]}],"source":["from huggingface_hub import HfApi, ModelCard, ModelCardData\n","from pathlib import Path\n","import os\n","import json\n","from datetime import datetime\n","\n","# Configuration\n","MODEL_NAME = \"yolov8n-mobile-phone\"\n","REPO_ID = f\"IndUSV/{MODEL_NAME}\"\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"  PUSHING MODEL TO HUGGING FACE\")\n","print(\"=\"*60)\n","print(f\"\\n  Target repository: {REPO_ID}\\n\")\n","\n","# Create the repo\n","api = HfApi()\n","\n","try:\n","    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True, private=False)\n","    print(f\"  Repository {REPO_ID} is ready\")\n","except Exception as e:\n","    print(f\"  Repository creation note: {str(e)}\")\n","\n","# Prepare model files\n","model_path = Path('/content/runs/detect/mobile_phone_detector/yolov8n_v1')\n","best_model_path = model_path / 'weights' / 'best.pt'\n","\n","if not best_model_path.exists():\n","    print(f\"\\n  Error: Model file not found at {best_model_path}\")\n","else:\n","    print(f\"\\n  Model file found: {best_model_path}\")\n","    print(f\"   Size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\\n\")\n","\n","    # Upload the model\n","    print(\"  Uploading model weights...\")\n","    try:\n","        api.upload_file(\n","            path_or_fileobj=str(best_model_path),\n","            path_in_repo=\"pytorch_model.bin\",\n","            repo_id=REPO_ID,\n","            repo_type=\"model\"\n","        )\n","        print(\"   Model weights uploaded!\\n\")\n","    except Exception as e:\n","        print(f\"  Upload error: {str(e)}\\n\")\n","\n","    # Upload training results and metrics\n","    results_file = model_path / 'results.csv'\n","    if results_file.exists():\n","        print(\"  Uploading training results...\")\n","        try:\n","            api.upload_file(\n","                path_or_fileobj=str(results_file),\n","                path_in_repo=\"results.csv\",\n","                repo_id=REPO_ID,\n","                repo_type=\"model\"\n","            )\n","            print(\"  Training results uploaded!\\n\")\n","        except Exception as e:\n","            print(f\"  Results upload note: {str(e)}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"21"},"source":["## Step 11: Create Model Card for Hugging Face"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"22","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186008522,"user_tz":-330,"elapsed":593,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"a1142c0d-91f3-4891-a4ee-9c02586d2334"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“ Creating and uploading model card...\n","âœ… Model card uploaded!\n","\n","============================================================\n","âœ… MODEL SUCCESSFULLY PUSHED TO HUGGING FACE\n","============================================================\n","\n","ğŸŒ Model URL: https://huggingface.co/IndUSV/yolov8n-mobile-phone\n","ğŸ“ Access your models: https://huggingface.co/IndUSV/models\n"]}],"source":["from huggingface_hub import HfApi\n","from datetime import datetime\n","\n","# Create a comprehensive model card\n","model_card_content = f\"\"\"\n","---\n","license: mit\n","tags:\n","- yolov8\n","- object-detection\n","- mobile-phone\n","- computer-vision\n","---\n","\n","# YOLOv8n Mobile Phone Detector\n","\n","A fine-tuned YOLOv8 Nano model trained on the Datacluster Labs Mobile Phone Image Dataset for object detection of mobile phones in images.\n","\n","## Model Details\n","\n","- **Base Model**: YOLOv8n (Nano)\n","- **Task**: Object Detection\n","- **Dataset**: Datacluster Labs Mobile Phone Image Dataset\n","- **Classes**: mobile_phone (1 class)\n","- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n","\n","## Usage\n","\n","```python\n","from ultralytics import YOLO\n","\n","# Load model from Hugging Face\n","model = YOLO('huggingface://IndUSV/yolov8n-mobile-phone-detector/pytorch_model.bin')\n","\n","# Run inference\n","results = model.predict(source='image.jpg', conf=0.5)\n","```\n","\n","## Training Details\n","\n","- **Input Size**: 640x640\n","- **Batch Size**: 16\n","- **Optimizer**: SGD\n","- **Learning Rate**: Auto\n","- **Epochs**: 50 (with early stopping)\n","- **Device**: NVIDIA GPU\n","\n","## Dataset Information\n","\n","The model was trained on the Datacluster Labs Mobile Phone Image Dataset which contains:\n","- High-resolution mobile phone images\n","- Pascal VOC format annotations\n","- Diverse backgrounds and lighting conditions\n","- Various phone models and orientations\n","\n","Dataset splits:\n","- Training: 80%\n","- Validation: 10%\n","- Testing: 10%\n","\n","## Performance\n","\n","Check results.csv for detailed training metrics and evaluation results.\n","\n","## Citation\n","\n","If you use this model, please cite:\n","- YOLOv8: https://github.com/ultralytics/ultralytics\n","- Dataset: https://www.kaggle.com/datasets/dataclusterlabs/mobile-phone-image-dataset\n","\n","## License\n","\n","This model is released under the MIT License.\n","\"\"\"\n","\n","# Upload model card\n","api = HfApi()\n","\n","print(\"\\n  Creating and uploading model card...\")\n","try:\n","    api.upload_file(\n","        path_or_fileobj=model_card_content.encode('utf-8'),\n","        path_in_repo=\"README.md\",\n","        repo_id=REPO_ID,\n","        repo_type=\"model\"\n","    )\n","    print(f\"  Model card uploaded!\\n\")\n","except Exception as e:\n","    print(f\". Model card note: {str(e)}\\n\")\n","\n","print(\"=\"*60)\n","print(\"  MODEL SUCCESSFULLY PUSHED TO HUGGING FACE\")\n","print(\"=\"*60)\n","print(f\"\\n  Model URL: https://huggingface.co/{REPO_ID}\")\n","print(f\"  Access your models: https://huggingface.co/IndUSV/models\")"]},{"cell_type":"markdown","metadata":{"id":"23"},"source":["## Step 12: Test Inference with Fine-tuned Model"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186109974,"user_tz":-330,"elapsed":455,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"834f9436-604c-4bc1-deb0-bac0439f498b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","ğŸ§ª TESTING INFERENCE\n","============================================================\n","\n","Running inference on 3 test images...\n","\n","Test 1: Datacluster Labs Phone Dataset (46).jpg\n","  ğŸ“± Detections: 1\n","    â€¢ Detection 1: Confidence 0.976\n","\n","Test 2: Datacluster Labs Phone Dataset (22).jpg\n","  ğŸ“± Detections: 1\n","    â€¢ Detection 1: Confidence 0.933\n","\n","Test 3: Datacluster Labs Phone Dataset (9).jpg\n","  ğŸ“± Detections: 1\n","    â€¢ Detection 1: Confidence 0.906\n","\n","============================================================\n","âœ… Inference test completed!\n","============================================================\n"]}],"source":["from ultralytics import YOLO\n","import os\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ğŸ§ª TESTING INFERENCE\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Load the best model\n","model = YOLO('/content/runs/detect/mobile_phone_detector/yolov8n_v1/weights/best.pt')\n","\n","# Run inference on test images\n","test_images = os.listdir(f\"{OUTPUT_DIR}/images/test\")[:3]  # Get first 3 test images\n","\n","print(f\"Running inference on {len(test_images)} test images...\\n\")\n","\n","for idx, img_file in enumerate(test_images, 1):\n","    img_path = os.path.join(OUTPUT_DIR, 'images', 'test', img_file)\n","\n","    # Run inference\n","    results = model.predict(source=img_path, conf=0.5, verbose=False)\n","\n","    # Display results\n","    result = results[0]\n","\n","    print(f\"Test {idx}: {img_file}\")\n","    print(f\"    Detections: {len(result.boxes)}\")\n","\n","    if len(result.boxes) > 0:\n","        for box_idx, box in enumerate(result.boxes, 1):\n","            conf = box.conf.item()\n","            print(f\"    â€¢ Detection {box_idx}: Confidence {conf:.3f}\")\n","    print()\n","\n","print(\"=\"*60)\n","print(\"  Inference test completed!\")\n","print(\"=\"*60)"]},{"cell_type":"markdown","metadata":{"id":"25"},"source":["## Step 13: Final Summary"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770186116087,"user_tz":-330,"elapsed":85,"user":{"displayName":"Vinay","userId":"13545643088616492399"}},"outputId":"cc660213-d831-4d5f-c4d7-46a1001f5bf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","â•”==========================================================â•—\n","â•‘          YOLO FINE-TUNING PIPELINE COMPLETED!            â•‘\n","â•š==========================================================â•\n","\n","âœ… COMPLETED STEPS:\n","  1. âœ“ Downloaded mobile phone dataset from Kaggle\n","  2. âœ“ Converted Pascal VOC annotations to YOLO format\n","  3. âœ“ Fine-tuned YOLOv8n model\n","  4. âœ“ Evaluated model performance\n","  5. âœ“ Pushed model to Hugging Face Hub\n","  6. âœ“ Created comprehensive model card\n","  7. âœ“ Tested inference on sample images\n","\n","ğŸ“Š MODEL INFORMATION:\n","  Model URL:    https://huggingface.co/IndUSV/yolov8n-mobile-phone\n","  Model ID:     IndUSV/yolov8n-mobile-phone\n","  Base Model:   YOLOv8n (Nano)\n","  Classes:      1 (mobile_phone)\n","  Framework:    Ultralytics YOLOv8\n","\n","ğŸ“ LOCAL ARTIFACTS:\n","  Training logs:  ./mobile_phone_detector/yolov8n_v1/\n","  Best weights:   ./mobile_phone_detector/yolov8n_v1/weights/best.pt\n","  Dataset:        /content/yolo_dataset\n","\n","ğŸš€ NEXT STEPS:\n","  1. Download model: model = YOLO('IndUSV/yolov8n-mobile-phone')\n","  2. Run inference: results = model.predict('image.jpg')\n","  3. Export model: model.export(format='onnx')\n","  4. Deploy on edge devices\n","\n","ğŸ“š DOCUMENTATION:\n","  YOLOv8:        https://docs.ultralytics.com/\n","  Dataset:       https://www.kaggle.com/datasets/dataclusterlabs/mobile-phone-image-dataset\n","  Hugging Face:  https://huggingface.co/IndUSV/models\n","\n","â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"]}],"source":["print(\"\\n\")\n","print(\"â•”\" + \"=\"*58 + \"â•—\")\n","print(\"â•‘\" + \" \"*10 + \"YOLO FINE-TUNING PIPELINE COMPLETED!\" + \" \"*12 + \"â•‘\")\n","print(\"â•š\" + \"=\"*58 + \"â•\")\n","\n","print(\"\\n  COMPLETED STEPS:\")\n","print(\"  1. âœ“ Downloaded mobile phone dataset from Kaggle\")\n","print(\"  2. âœ“ Converted Pascal VOC annotations to YOLO format\")\n","print(\"  3. âœ“ Fine-tuned YOLOv8n model\")\n","print(\"  4. âœ“ Evaluated model performance\")\n","print(\"  5. âœ“ Pushed model to Hugging Face Hub\")\n","print(\"  6. âœ“ Created comprehensive model card\")\n","print(\"  7. âœ“ Tested inference on sample images\")\n","\n","print(f\"\\n  MODEL INFORMATION:\")\n","print(f\"  Model URL:    https://huggingface.co/{REPO_ID}\")\n","print(f\"  Model ID:     {REPO_ID}\")\n","print(f\"  Base Model:   YOLOv8n (Nano)\")\n","print(f\"  Classes:      1 (mobile_phone)\")\n","print(f\"  Framework:    Ultralytics YOLOv8\")\n","\n","print(f\"\\n  LOCAL ARTIFACTS:\")\n","print(f\"  Training logs:  ./mobile_phone_detector/yolov8n_v1/\")\n","print(f\"  Best weights:   ./mobile_phone_detector/yolov8n_v1/weights/best.pt\")\n","print(f\"  Dataset:        {OUTPUT_DIR}\")\n","\n","print(f\"\\n  NEXT STEPS:\")\n","print(f\"  1. Download model: model = YOLO('IndUSV/{MODEL_NAME}')\")\n","print(f\"  2. Run inference: results = model.predict('image.jpg')\")\n","print(f\"  3. Export model: model.export(format='onnx')\")\n","print(f\"  4. Deploy on edge devices\")\n","\n","print(f\"\\n  DOCUMENTATION:\")\n","print(f\"  YOLOv8:        https://docs.ultralytics.com/\")\n","print(f\"  Dataset:       https://www.kaggle.com/datasets/dataclusterlabs/mobile-phone-image-dataset\")\n","print(f\"  Hugging Face:  https://huggingface.co/IndUSV/models\")\n","\n","print(\"\\n\" + \"â•\"*60)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a367a0df4ab94c5e9628ec7934283b09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4f7bea897e74c6c80ec4033a6a61884","IPY_MODEL_ecc1ceb4d8eb4966b46d3ecb2621dcd2","IPY_MODEL_0186aacbfcca4cf7910f40e7341fe4b7"],"layout":"IPY_MODEL_372915d321f846bd89793785e56830fb"}},"b4f7bea897e74c6c80ec4033a6a61884":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47aa8b94d852435b897f396850ae6ff9","placeholder":"â€‹","style":"IPY_MODEL_9d80e494f22a41219167cf7098725a8c","value":"Processingâ€‡Filesâ€‡(1â€‡/â€‡1)â€‡â€‡â€‡â€‡â€‡â€‡:â€‡100%"}},"ecc1ceb4d8eb4966b46d3ecb2621dcd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04629e83ef064c948cbba22ad4b9baf4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b84218ad2f349fca590af58c07527a5","value":1}},"0186aacbfcca4cf7910f40e7341fe4b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7076d7a07c6c4763b7b2974bd9be1f2a","placeholder":"â€‹","style":"IPY_MODEL_ad0029a2cdde41a599f705ade3b68b3f","value":"â€‡6.25MBâ€‡/â€‡6.25MB,â€‡â€‡0.00B/sâ€‡â€‡"}},"372915d321f846bd89793785e56830fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47aa8b94d852435b897f396850ae6ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d80e494f22a41219167cf7098725a8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04629e83ef064c948cbba22ad4b9baf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"2b84218ad2f349fca590af58c07527a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7076d7a07c6c4763b7b2974bd9be1f2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0029a2cdde41a599f705ade3b68b3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b9ca9c8680449a8a5bca9e45b077cfc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9022cd2975f5457e81bbb059ddac0d67","IPY_MODEL_f21d899641ed4b2eac4a3502620d80ba","IPY_MODEL_2372caa2578a477fa59c6cd0f6a85492"],"layout":"IPY_MODEL_b4adbb1cd5cf4a56a1a58af84db51aa6"}},"9022cd2975f5457e81bbb059ddac0d67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98ea7305da3c4efe990ec8758f4f970c","placeholder":"â€‹","style":"IPY_MODEL_2f964136c97944aa9dd2f087aaf82b20","value":"Newâ€‡Dataâ€‡Uploadâ€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡:â€‡"}},"f21d899641ed4b2eac4a3502620d80ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aad73b3357a7499b9c34bb337f98540f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8097174c047b46cfbaaf1faa162ab6aa","value":0}},"2372caa2578a477fa59c6cd0f6a85492":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a242514f57f64f03bafab092a98d6cdf","placeholder":"â€‹","style":"IPY_MODEL_0682d5112f7742ac987d0240f109b468","value":"â€‡â€‡0.00Bâ€‡/â€‡â€‡0.00B,â€‡â€‡0.00B/sâ€‡â€‡"}},"b4adbb1cd5cf4a56a1a58af84db51aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98ea7305da3c4efe990ec8758f4f970c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f964136c97944aa9dd2f087aaf82b20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aad73b3357a7499b9c34bb337f98540f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8097174c047b46cfbaaf1faa162ab6aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a242514f57f64f03bafab092a98d6cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0682d5112f7742ac987d0240f109b468":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d2f65d7973b45458347a9117a83e4f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ea74d0245704c17892cb6318c558e88","IPY_MODEL_bc661a70db0f4f37941ca4b61e2db75a","IPY_MODEL_e2796ea47f8c4ee7bd1ba68acb3cc7cd"],"layout":"IPY_MODEL_c44ba661e11b4604b9398dd64058e00a"}},"3ea74d0245704c17892cb6318c558e88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04e75a8c97bb4368b4f38aafa162afcb","placeholder":"â€‹","style":"IPY_MODEL_311d3fc85e6c4cc8bedf5f9bd21c0e03","value":"â€‡â€‡...olov8n_v1/weights/best.pt:â€‡100%"}},"bc661a70db0f4f37941ca4b61e2db75a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46093716481946f9a92a1522de7788ed","max":6248938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4871ff809b8f4d8c9d7cb066ec5f0698","value":6248938}},"e2796ea47f8c4ee7bd1ba68acb3cc7cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b0379ae8294489d99c25c7be3f4a675","placeholder":"â€‹","style":"IPY_MODEL_dbe8d84e0b36441192323fdb8426f019","value":"â€‡6.25MBâ€‡/â€‡6.25MBâ€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡â€‡"}},"c44ba661e11b4604b9398dd64058e00a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04e75a8c97bb4368b4f38aafa162afcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"311d3fc85e6c4cc8bedf5f9bd21c0e03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46093716481946f9a92a1522de7788ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4871ff809b8f4d8c9d7cb066ec5f0698":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b0379ae8294489d99c25c7be3f4a675":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbe8d84e0b36441192323fdb8426f019":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}